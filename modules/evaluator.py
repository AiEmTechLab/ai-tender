# modules/evaluator.py
import streamlit as st
import pandas as pd
import json, re, os
from groq import Groq
from dotenv import load_dotenv
from langdetect import detect
from deep_translator import GoogleTranslator
from modules.extractors import extract_text_with_pages  # Ø§Ù„ØªØ­Ø¯ÙŠØ« Ù‡Ù†Ø§

# ØªØ­Ù…ÙŠÙ„ Ù…ÙØªØ§Ø­ Groq Ù…Ù† .env
load_dotenv()
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ===========================================================
# ğŸ”¤ Ø¯Ø§Ù„Ø© Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù„ØºØ© ÙˆØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
# ===========================================================
def translate_if_needed(criteria_list, text):
    """Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ø±Ø¶ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ ØªÙØªØ±Ø¬Ù… Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"""
    try:
        sample = text[:1000]
        lang = detect(sample)
        if lang == "en":
            st.info("ğŸ”¤ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£Ù† Ø§Ù„Ø¹Ø±Ø¶ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ ÙŠØªÙ… Ø§Ù„Ø¢Ù† ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§...")
            translated = [
                GoogleTranslator(source="ar", target="en").translate(c)
                for c in criteria_list
            ]
            return translated, "en"
        else:
            return criteria_list, "ar"
    except Exception as e:
        st.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ© Ø¨Ø¯Ù‚Ø© ({e})ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± ÙƒÙ…Ø§ Ù‡ÙŠ.")
        return criteria_list, "ar"


# ===========================================================
# ğŸ§  Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
# ===========================================================
@st.cache_data(show_spinner=False)
def evaluate_offers(offers, criteria_list):
    results, details = [], {}

    for f in offers:
        with st.spinner(f"ğŸ” ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¶: {f.name}"):

            # âœ… ØªØ­Ø¯ÙŠØ«: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ extract_text_with_pages
            data = extract_text_with_pages(f)
            if isinstance(data, dict):
                if data.get("type") == "pdf":
                    text = "\n".join(p["text"] for p in data.get("pages", []))
                elif data.get("type") == "docx":
                    text = data.get("text", "")
                else:
                    text = ""
            else:
                text = str(data)

            if not text.strip():
                st.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ù…Ù† Ø§Ù„Ù…Ù„Ù: {f.name}")
                continue

            # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø¥Ù† Ù„Ø²Ù…
            criteria_list, lang_detected = translate_if_needed(criteria_list, text)
            text_criteria = "\n".join([f"- {c}" for c in criteria_list])

            # ===== Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (Prompt) =====
            prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ø¹Ø±ÙˆØ¶ ÙÙ†ÙŠØ© ÙˆØªÙ‚Ù†ÙŠØ©.
Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø§Ù„Ù…Ø£Ø®ÙˆØ° Ù…Ù† Ø¹Ø±Ø¶ ÙÙ†ÙŠØŒ Ø«Ù… Ù‚ÙŠÙ‘Ù… Ø§Ù„Ø¹Ø±Ø¶ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØ§Ù„ÙŠØ©:

Ù„ÙƒÙ„ Ù…Ø¹ÙŠØ§Ø±:
- Ø¶Ø¹ Ø¯Ø±Ø¬Ø© Ù…Ù† 1 Ø¥Ù„Ù‰ 4 (1 = Ø¶Ø¹ÙŠÙØŒ 4 = Ù…Ù…ØªØ§Ø²)
- Ø§ÙƒØªØ¨ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ Ø·Ø±Ø­ØªÙ‡ Ø¹Ù„Ù‰ Ù†ÙØ³Ùƒ Ù„ØªÙ‚ÙŠÙŠÙ…Ù‡ (ai_question)
- Ø§Ø´Ø±Ø­ Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ù„Ù„Ø¯Ø±Ø¬Ø© (reason)

Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±ØŒ Ø£Ø¶Ù ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù…ÙØªØ§Ø­Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ø¨Ø§Ø³Ù…:
"overall_comment": ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ø§Ù…Ø© ÙˆØ´Ø§Ù…Ù„Ø© Ø¹Ù† Ø§Ù„Ø¹Ø±Ø¶.

Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· ÙƒØ§Ù„ØªØ§Ù„ÙŠ:
{{
  "scores": [
    {{
      "criterion": "Ø§Ø³Ù… Ø§Ù„Ù…Ø¹ÙŠØ§Ø±",
      "score": Ø±Ù‚Ù… Ù…Ù† 1 Ø¥Ù„Ù‰ 4,
      "ai_question": "Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ Ø·Ø±Ø­Ù‡ Ø§Ù„Ù…Ù‚ÙŠÙ…",
      "reason": "Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ù„Ù„ØªÙ‚ÙŠÙŠÙ…"
    }}
  ],
  "overall_comment": "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ø§Ù…Ø© Ø¹Ù† Ø§Ù„Ø¹Ø±Ø¶ ÙƒÙƒÙ„"
}}

Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±:
{text_criteria}

Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ÙÙ†ÙŠ (Ø¨Ø¯ÙˆÙ† Ø§Ø®ØªØµØ§Ø±):
{text[:20000]}

Ø±Ø¬Ø§Ø¡Ù‹ Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.
"""

            try:
                response = client.chat.completions.create(
                    model="llama-3.1-8b-instant",
                    temperature=0.3,
                    max_tokens=3500,
                    messages=[{"role": "user", "content": prompt}],
                )
                result_text = response.choices[0].message.content.strip()
                print("ğŸ§  Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:\n", result_text[:1000])

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON Ù…Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø©
                json_match = re.search(r"\{.*\}", result_text, re.S)
                if json_match:
                    data = json.loads(json_match.group(0))
                    scores = data.get("scores", [])
                    comment = data.get("overall_comment", "â€” Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ø§Ù…Ø© â€”")

                    df = pd.DataFrame(scores)
                    for col in ["criterion", "score", "reason", "ai_question"]:
                        if col not in df.columns:
                            df[col] = "â€”"

                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨Ø©
                    df["score"] = pd.to_numeric(df["score"], errors="coerce").fillna(0)
                    overall = df["score"].mean() / 4  # Ù…Ù† 0 Ø¥Ù„Ù‰ 1

                    results.append(
                        {"file": f.name, "overall": overall, "comment": comment}
                    )
                    details[f.name] = df
                else:
                    st.warning(f"âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù… ÙŠÙØ±Ø¬Ø¹ JSON ØµØ§Ù„Ø­ Ù„Ù„Ù…Ù„Ù: {f.name}")

            except Exception as e:
                st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ù„Ù {f.name}: {e}")

    # ===== ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame =====
    if results:
        ranked = pd.DataFrame(results).sort_values("overall", ascending=False)
        ranked.reset_index(drop=True, inplace=True)
        return ranked, details
    else:
        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªÙ‚ÙŠÙŠÙ… Ø£ÙŠ Ù…Ù† Ø§Ù„Ø¹Ø±ÙˆØ¶.")
        return pd.DataFrame(), {}
