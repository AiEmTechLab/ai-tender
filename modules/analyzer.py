# modules/analyzer.py
import os, json, hashlib, re
import streamlit as st
from groq import Groq

# ============================================================
# â˜ï¸ Ø¥Ø¹Ø¯Ø§Ø¯ Groq (Ø³Ø­Ø§Ø¨ÙŠ ÙÙ‚Ø·)
# ============================================================
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None

def _md5(s: str) -> str:
    return hashlib.md5(s.encode("utf-8", "ignore")).hexdigest()

@st.cache_data(show_spinner=False)
def _llm_json_only(prompt: str) -> str:
    """ÙŠØ³ØªØ¯Ø¹ÙŠ Groq ÙˆÙŠØ¹ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù†ØµÙŠØ© (ÙŠØªÙˆÙ‚Ø¹ JSON ÙÙ‚Ø·)."""
    if not client:
        raise RuntimeError("âš ï¸ GROQ_API_KEY ØºÙŠØ± Ù…Ø¶Ø¨ÙˆØ·.")
    resp = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.25,
    )
    return resp.choices[0].message.content.strip()

def _safe_json_loads(s: str):
    """ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON Ø­ØªÙ‰ Ù„Ùˆ Ø£Ø¶Ø§Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù†ØµÙˆØµØ§Ù‹ Ø²Ø§Ø¦Ø¯Ø©."""
    try:
        return json.loads(s)
    except Exception:
        m = re.search(r"(\[.*\])", s, re.DOTALL)
        if m:
            try:
                return json.loads(m.group(1))
            except:
                pass
    return None

# ============================================================
# ğŸ’¾ Ù†Ø¸Ø§Ù… ÙƒØ§Ø´ Ù„Ù„ØªØ±Ø¬Ù…Ø§Øª (Ø¯Ø§Ø¦Ù… ÙˆÙ…Ø­Ù„ÙŠ)
# ============================================================
CACHE_DIR = "cache_translations"
CACHE_FILE = os.path.join(CACHE_DIR, "translations.json")
os.makedirs(CACHE_DIR, exist_ok=True)

def _load_cache():
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def _save_cache(cache):
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

@st.cache_data(show_spinner=False)
def translate_with_cache(text: str) -> str:
    """ğŸŒ ØªØ±Ø¬Ù…Ø© Ø°ÙƒÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ ÙƒØ§Ø´ Ø¯Ø§Ø¦Ù…"""
    text_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
    cache = _load_cache()

    if text_hash in cache:
        return cache[text_hash]

    st.info("ğŸŒ ÙŠØªÙ… Ø§Ù„Ø¢Ù† ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)...")
    prompt = f"ØªØ±Ø¬Ù… Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ØªØ±Ø¬Ù…Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø¨Ø¯ÙˆÙ† Ø­Ø°Ù Ø£Ùˆ Ø§Ø®ØªØµØ§Ø±:\n{text[:20000]}"

    response = client.chat.completions.create(
        model="llama-3.3-70b-versatile",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
    )

    translated = response.choices[0].message.content.strip()
    cache[text_hash] = translated
    _save_cache(cache)
    return translated

# ============================================================
# ğŸ“„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù…Ø¹ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª
# ============================================================
def analyze_sections_with_pages(doc_payload: dict):
    """
    doc_payload:
      - PDF: {"type":"pdf","pages":[{"page_num":1,"text":"..."}, ...]}
      - DOCX: {"type":"docx","text":"..."}
    ÙŠØ¹ÙŠØ¯ Ù‚Ø§Ø¦Ù…Ø© Ø£Ù‚Ø³Ø§Ù… JSON:
    [
      {"section":"...","summary":"...","start_page":1,"content":"..."}
    ]
    """
    st.info("â˜ï¸ Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¹Ø¨Ø± Groqâ€¦")

    if doc_payload.get("type") == "pdf":
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØµÙØ­Ø©
        parts = [f"[[PAGE:{p['page_num']}]]\n{p['text']}" for p in doc_payload["pages"]]
        full = "\n\n".join(parts)
        clipped = full[:20000]

        prompt = f"""
Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ (Ø¹Ø±Ø¶ ÙÙ†ÙŠ) Ø§Ù„Ù…Ø±Ø³Ù„ Ù…Ø¹ Ø¹Ù„Ø§Ù…Ø§Øª ØµÙØ­Ø§Øª Ø¨Ø§Ù„Ø´ÙƒÙ„ [[PAGE:n]].
Ù‚Ø³Ù‘Ù…Ù‡ Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø«Ù„: Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©ØŒ Ø§Ù„Ø£Ù‡Ø¯Ø§ÙØŒ ÙÙ‡Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŒ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©ØŒ Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ØŒ Ø§Ù„ÙØ±ÙŠÙ‚ØŒ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ù„Ø®Ø§ØªÙ…Ø©.
Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø®Ø§Ø±Ø¬Ù‡:

[
  {{
    "section": "Ø§Ø³Ù… Ø§Ù„Ù‚Ø³Ù…",
    "summary": "Ù…Ù„Ø®Øµ Ø§Ù„Ù‚Ø³Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    "start_page": Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªÙŠ ÙŠØ¨Ø¯Ø£ Ø¹Ù†Ø¯Ù‡Ø§ Ø§Ù„Ù‚Ø³Ù… (Ø§Ø³ØªØ¯Ù„ Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ù† [[PAGE:n]]),
    "content": "Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù‚Ø³Ù… ÙƒÙ…Ø§ Ù‡Ùˆ Ø¯ÙˆÙ† Ø­Ø°Ù Ø£Ùˆ Ø§Ø®ØªØµØ§Ø±ØŒ ÙˆØ§Ø¬Ù…Ø¹ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ù…ØªØµÙ„Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØªØ§Ø¨Ø¹Ø©"
  }}
]

Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ù„Ù‘Ù… Ø¨Ø§Ù„ØµÙØ­Ø§Øª:
-----------------------
{clipped}
"""
        reply = _llm_json_only(prompt)
        data = _safe_json_loads(reply)

        if not data:
            st.warning("âš ï¸ Ù„Ù… ÙŠÙØ±Ø¬Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ JSON ØµØ§Ù„Ø­Ù‹Ø§ â€” ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„Ù‹Ø§ ÙƒÙ‚Ø³Ù… ÙˆØ§Ø­Ø¯.")
            return [{
                "section": "Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„",
                "summary": "ØªØ­Ù„ÙŠÙ„ Ø¹Ø§Ù…",
                "start_page": 1,
                "content": reply
            }]

        out = []
        for item in data:
            out.append({
                "section": str(item.get("section", "")).strip(),
                "summary": str(item.get("summary", "")).strip(),
                "start_page": int(item.get("start_page", 1) or 1),
                "content": str(item.get("content", "")).strip(),
            })
        return out

    elif doc_payload.get("type") == "docx":
        full = doc_payload["text"]
        clipped = full[:20000]
        prompt = f"""
Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ (Ø¹Ø±Ø¶ ÙÙ†ÙŠ). Ù‚Ø³Ù‘Ù…Ù‡ Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø«Ù„: Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©ØŒ Ø§Ù„Ø£Ù‡Ø¯Ø§ÙØŒ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©ØŒ Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ØŒ Ø§Ù„ÙØ±ÙŠÙ‚ØŒ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ù„Ø®Ø§ØªÙ…Ø©.
Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
[
  {{
    "section": "Ø§Ø³Ù… Ø§Ù„Ù‚Ø³Ù…",
    "summary": "Ù…Ù„Ø®Øµ Ø§Ù„Ù‚Ø³Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    "start_page": 1,
    "content": "Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù‚Ø³Ù…"
  }}
]
Ø§Ù„Ù†Øµ:
------
{clipped}
"""
        reply = _llm_json_only(prompt)
        data = _safe_json_loads(reply)
        if not data:
            return [{
                "section": "Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„",
                "summary": "ØªØ­Ù„ÙŠÙ„ Ø¹Ø§Ù…",
                "start_page": 1,
                "content": reply
            }]
        out = []
        for item in data:
            out.append({
                "section": str(item.get("section", "")).strip(),
                "summary": str(item.get("summary", "")).strip(),
                "start_page": 1,
                "content": str(item.get("content", "")).strip(),
            })
        return out

    else:
        st.error("ØµÙŠØºØ© Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©.")
        return []

# ============================================================
# ğŸ§  Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ø±ÙˆØ¶
# ============================================================
def analyze_all_offers(offers):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… + Ø§Ù„ØªØ±Ø¬Ù…Ø© + Ø§Ù„ÙƒØ§Ø´"""
    st.info("ğŸ¤– Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø³Ø­Ø§Ø¨ÙŠÙ‹Ø§ Ø¹Ø¨Ø± Groq...")
    from modules.extractors import extract_text
    topics_data = {}

    for offer in offers:
        st.markdown(f"ğŸ“‚ **Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¶:** {offer.name}")
        try:
            full_text = extract_text(offer)
            if not full_text or len(full_text.strip()) < 200:
                st.warning(f"âš ï¸ Ø§Ù„Ù†Øµ ÙÙŠ {offer.name} ÙØ§Ø±Øº Ø£Ùˆ Ù‚ØµÙŠØ± Ø¬Ø¯Ù‹Ø§")
                continue

            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø­Ù…ÙˆÙ„Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
            if offer.name.lower().endswith(".pdf"):
                from modules.extractors import fitz
                doc = fitz.open(stream=offer.read(), filetype="pdf")
                offer.seek(0)
                pages = [{"page_num": i+1, "text": p.get_text("text")} for i, p in enumerate(doc)]
                payload = {"type": "pdf", "pages": pages}
            else:
                payload = {"type": "docx", "text": full_text}

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
            data = analyze_sections_with_pages(payload)
            topics_data[offer.name] = data
            st.success(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ {offer.name} Ø¨Ù†Ø¬Ø§Ø­.")

        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ {offer.name}: {e}")

    st.session_state.topics = topics_data
    st.success("ğŸ¯ ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ø±ÙˆØ¶ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù‚Ø³Ø§Ù….")

